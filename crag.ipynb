{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class PrepEvent(Event):\n",
    "    \"\"\"Prep event (prepares for retrieval).\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
    "\n",
    "    retrieved_nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RelevanceEvalEvent(Event):\n",
    "    \"\"\"Relevance evaluation event (gets results of relevance evaluation).\"\"\"\n",
    "\n",
    "    relevant_results: list[str]\n",
    "\n",
    "\n",
    "class TextExtractEvent(Event):\n",
    "    \"\"\"Text extract event. Extracts relevant text and concatenates.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "    search_text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    ")\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    PromptTemplate,\n",
    "    SummaryIndex,\n",
    ")\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "\n",
    "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class CorrectiveRAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n",
    "        documents: list[Document] | None = ev.get(\"documents\")\n",
    "\n",
    "        if documents is None:\n",
    "            return None\n",
    "\n",
    "        embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def prepare_for_retrieval(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> PrepEvent | None:\n",
    "        \"\"\"Prepare for retrieval.\"\"\"\n",
    "\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        tavily_ai_apikey: str | None = ev.get(\"tavily_ai_apikey\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        llm = Ollama(model=\"llama2:7b\", request_timeout=60)\n",
    "        await ctx.set(\n",
    "            \"relevancy_pipeline\",\n",
    "            QueryPipeline(chain=[DEFAULT_RELEVANCY_PROMPT_TEMPLATE, llm]),\n",
    "        )\n",
    "        await ctx.set(\n",
    "            \"transform_query_pipeline\",\n",
    "            QueryPipeline(chain=[DEFAULT_TRANSFORM_QUERY_TEMPLATE, llm]),\n",
    "        )\n",
    "\n",
    "        await ctx.set(\"llm\", llm)\n",
    "        await ctx.set(\"index\", index)\n",
    "        await ctx.set(\"tavily_tool\", TavilyToolSpec(api_key=tavily_ai_apikey))\n",
    "\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> RetrieveEvent | None:\n",
    "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "        retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        index = await ctx.get(\"index\", default=None)\n",
    "        tavily_tool = await ctx.get(\"tavily_tool\", default=None)\n",
    "        if not (index or tavily_tool):\n",
    "            raise ValueError(\n",
    "                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
    "            )\n",
    "\n",
    "        retriever: BaseRetriever = index.as_retriever(**retriever_kwargs)\n",
    "        result = retriever.retrieve(query_str)\n",
    "        await ctx.set(\"retrieved_nodes\", result)\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        return RetrieveEvent(retrieved_nodes=result)\n",
    "\n",
    "    @step\n",
    "    async def eval_relevance(\n",
    "        self, ctx: Context, ev: RetrieveEvent\n",
    "    ) -> RelevanceEvalEvent:\n",
    "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        relevancy_results = []\n",
    "        for node in retrieved_nodes:\n",
    "            relevancy_pipeline = await ctx.get(\"relevancy_pipeline\")\n",
    "            relevancy = relevancy_pipeline.run(\n",
    "                context_str=node.text, query_str=query_str\n",
    "            )\n",
    "            relevancy_results.append(relevancy.message.content.lower().strip())\n",
    "\n",
    "        await ctx.set(\"relevancy_results\", relevancy_results)\n",
    "        return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
    "\n",
    "    @step\n",
    "    async def extract_relevant_texts(\n",
    "        self, ctx: Context, ev: RelevanceEvalEvent\n",
    "    ) -> TextExtractEvent:\n",
    "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
    "        retrieved_nodes = await ctx.get(\"retrieved_nodes\")\n",
    "        relevancy_results = ev.relevant_results\n",
    "\n",
    "        relevant_texts = [\n",
    "            retrieved_nodes[i].text\n",
    "            for i, result in enumerate(relevancy_results)\n",
    "            if result == \"yes\"\n",
    "        ]\n",
    "\n",
    "        result = \"\\n\".join(relevant_texts)\n",
    "        return TextExtractEvent(relevant_text=result)\n",
    "\n",
    "    @step\n",
    "    async def transform_query_pipeline(\n",
    "        self, ctx: Context, ev: TextExtractEvent\n",
    "    ) -> QueryEvent:\n",
    "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        relevancy_results = await ctx.get(\"relevancy_results\")\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        # If any document is found irrelevant, transform the query string for better search results.\n",
    "        if \"no\" in relevancy_results:\n",
    "            qp = await ctx.get(\"transform_query_pipeline\")\n",
    "            transformed_query_str = qp.run(query_str=query_str).message.content\n",
    "            # Conduct a search with the transformed query string and collect the results.\n",
    "            tavily_tool = await ctx.get(\"tavily_tool\")\n",
    "            search_results = tavily_tool.search(\n",
    "                transformed_query_str, max_results=5\n",
    "            )\n",
    "            search_text = \"\\n\".join([result.text for result in search_results])\n",
    "        else:\n",
    "            search_text = \"\"\n",
    "\n",
    "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
    "\n",
    "    @step\n",
    "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        search_text = ev.search_text\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
    "        index = SummaryIndex.from_documents(documents)\n",
    "        query_engine = index.as_query_engine()\n",
    "        result = query_engine.query(query_str)\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "workflow = CorrectiveRAGWorkflow()\n",
    "index = await workflow.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'eval_relevance': llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:304\u001b[39m, in \u001b[36mWorkflow._start.<locals>._task\u001b[39m\u001b[34m(name, queue, step, config)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mCorrectiveRAGWorkflow.eval_relevance\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    137\u001b[39m relevancy_pipeline = \u001b[38;5;28;01mawait\u001b[39;00m ctx.get(\u001b[33m\"\u001b[39m\u001b[33mrelevancy_pipeline\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m relevancy = \u001b[43mrelevancy_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m relevancy_results.append(relevancy.message.content.lower().strip())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:413\u001b[39m, in \u001b[36mQueryPipeline.run\u001b[39m\u001b[34m(self, return_values_direct, callback_manager, batch, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    411\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_payload}\n\u001b[32m    412\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     outputs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_values_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_values_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:781\u001b[39m, in \u001b[36mQueryPipeline._run\u001b[39m\u001b[34m(self, return_values_direct, show_intermediates, batch, *args, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     result_output_dicts, intermediate_dicts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43mroot_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_intermediates\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_single_result_output(\n\u001b[32m    787\u001b[39m             result_output_dicts, return_values_direct\n\u001b[32m    788\u001b[39m         ),\n\u001b[32m    789\u001b[39m         intermediate_dicts,\n\u001b[32m    790\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:938\u001b[39m, in \u001b[36mQueryPipeline._run_multi\u001b[39m\u001b[34m(self, module_input_dict, show_intermediates)\u001b[39m\n\u001b[32m    937\u001b[39m     print_debug_input(module_key, module_input)\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m output_dict = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    941\u001b[39m     show_intermediates\n\u001b[32m    942\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m module_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m run_state.intermediate_outputs\n\u001b[32m    943\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:203\u001b[39m, in \u001b[36mQueryComponent.run_component\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m.validate_component_inputs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m component_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.validate_component_outputs(component_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:990\u001b[39m, in \u001b[36mLLMChatComponent._run_component\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: response}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:173\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:322\u001b[39m, in \u001b[36mOllama.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\ollama\\_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03mCreate a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03mReturns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m  \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\ollama\\_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\ollama\\_client.py:122\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n",
      "\u001b[31mResponseError\u001b[39m: llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer (status code: 500)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[32m      3\u001b[39m tavily_ai_api_key = \u001b[33m\"\u001b[39m\u001b[33mtvly-dev-9L2W3WEagujeArELnYobzM82CedCLSKB\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m workflow.run(\n\u001b[32m      6\u001b[39m     query_str=\u001b[33m\"\u001b[39m\u001b[33mHow was Llama2 pretrained?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     index=index,\n\u001b[32m      8\u001b[39m     tavily_ai_apikey=tavily_ai_api_key,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m display(Markdown(\u001b[38;5;28mstr\u001b[39m(response)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:542\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    539\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    540\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    545\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    546\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:311\u001b[39m, in \u001b[36mWorkflow._start.<locals>._task\u001b[39m\u001b[34m(name, queue, step, config)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    312\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    315\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    316\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    317\u001b[39m     )\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'eval_relevance': llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer (status code: 500)"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "tavily_ai_api_key = \"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\"\n",
    "\n",
    "response = await workflow.run(\n",
    "    query_str=\"How was Llama2 pretrained?\",\n",
    "    index=index,\n",
    "    tavily_ai_apikey=tavily_ai_api_key,\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await workflow.run(\n",
    "    query_str=\"What is the functionality of latest ChatGPT memory.\"\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
