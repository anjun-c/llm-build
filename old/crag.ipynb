{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class PrepEvent(Event):\n",
    "    \"\"\"Prep event (prepares for retrieval).\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
    "\n",
    "    retrieved_nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RelevanceEvalEvent(Event):\n",
    "    \"\"\"Relevance evaluation event (gets results of relevance evaluation).\"\"\"\n",
    "\n",
    "    relevant_results: list[str]\n",
    "\n",
    "\n",
    "class TextExtractEvent(Event):\n",
    "    \"\"\"Text extract event. Extracts relevant text and concatenates.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "    search_text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    ")\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    PromptTemplate,\n",
    "    SummaryIndex,\n",
    ")\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "\n",
    "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class CorrectiveRAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n",
    "        documents: list[Document] | None = ev.get(\"documents\")\n",
    "\n",
    "        if documents is None:\n",
    "            return None\n",
    "\n",
    "        embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def prepare_for_retrieval(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> PrepEvent | None:\n",
    "        \"\"\"Prepare for retrieval.\"\"\"\n",
    "\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        tavily_ai_apikey: str | None = ev.get(\"tavily_ai_apikey\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        llm = Ollama(model=\"llama2:7b\", request_timeout=60)\n",
    "        await ctx.set(\n",
    "            \"relevancy_pipeline\",\n",
    "            QueryPipeline(chain=[DEFAULT_RELEVANCY_PROMPT_TEMPLATE, llm]),\n",
    "        )\n",
    "        await ctx.set(\n",
    "            \"transform_query_pipeline\",\n",
    "            QueryPipeline(chain=[DEFAULT_TRANSFORM_QUERY_TEMPLATE, llm]),\n",
    "        )\n",
    "\n",
    "        await ctx.set(\"llm\", llm)\n",
    "        await ctx.set(\"index\", index)\n",
    "        await ctx.set(\"tavily_tool\", TavilyToolSpec(api_key=tavily_ai_apikey))\n",
    "\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> RetrieveEvent | None:\n",
    "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "        retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        index = await ctx.get(\"index\", default=None)\n",
    "        tavily_tool = await ctx.get(\"tavily_tool\", default=None)\n",
    "        if not (index or tavily_tool):\n",
    "            raise ValueError(\n",
    "                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
    "            )\n",
    "\n",
    "        retriever: BaseRetriever = index.as_retriever(**retriever_kwargs)\n",
    "        result = retriever.retrieve(query_str)\n",
    "        await ctx.set(\"retrieved_nodes\", result)\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        return RetrieveEvent(retrieved_nodes=result)\n",
    "\n",
    "    @step\n",
    "    async def eval_relevance(\n",
    "        self, ctx: Context, ev: RetrieveEvent\n",
    "    ) -> RelevanceEvalEvent:\n",
    "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        relevancy_results = []\n",
    "        for node in retrieved_nodes:\n",
    "            relevancy_pipeline = await ctx.get(\"relevancy_pipeline\")\n",
    "            relevancy = relevancy_pipeline.run(\n",
    "                context_str=node.text, query_str=query_str\n",
    "            )\n",
    "            relevancy_results.append(relevancy.message.content.lower().strip())\n",
    "\n",
    "        await ctx.set(\"relevancy_results\", relevancy_results)\n",
    "        return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
    "\n",
    "    @step\n",
    "    async def extract_relevant_texts(\n",
    "        self, ctx: Context, ev: RelevanceEvalEvent\n",
    "    ) -> TextExtractEvent:\n",
    "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
    "        retrieved_nodes = await ctx.get(\"retrieved_nodes\")\n",
    "        relevancy_results = ev.relevant_results\n",
    "\n",
    "        relevant_texts = [\n",
    "            retrieved_nodes[i].text\n",
    "            for i, result in enumerate(relevancy_results)\n",
    "            if result == \"yes\"\n",
    "        ]\n",
    "\n",
    "        result = \"\\n\".join(relevant_texts)\n",
    "        return TextExtractEvent(relevant_text=result)\n",
    "\n",
    "    @step\n",
    "    async def transform_query_pipeline(\n",
    "        self, ctx: Context, ev: TextExtractEvent\n",
    "    ) -> QueryEvent:\n",
    "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        relevancy_results = await ctx.get(\"relevancy_results\")\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        # If any document is found irrelevant, transform the query string for better search results.\n",
    "        if \"no\" in relevancy_results:\n",
    "            qp = await ctx.get(\"transform_query_pipeline\")\n",
    "            transformed_query_str = qp.run(query_str=query_str).message.content\n",
    "            # Conduct a search with the transformed query string and collect the results.\n",
    "            tavily_tool = await ctx.get(\"tavily_tool\")\n",
    "            search_results = tavily_tool.search(\n",
    "                transformed_query_str, max_results=5\n",
    "            )\n",
    "            search_text = \"\\n\".join([result.text for result in search_results])\n",
    "        else:\n",
    "            search_text = \"\"\n",
    "\n",
    "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
    "\n",
    "    @step\n",
    "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        search_text = ev.search_text\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
    "        index = SummaryIndex.from_documents(documents)\n",
    "        query_engine = index.as_query_engine()\n",
    "        result = query_engine.query(query_str)\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "workflow = CorrectiveRAGWorkflow()\n",
    "index = await workflow.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'query_result': \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:42\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     41\u001b[39m     llm = OpenAI()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\llms\\openai\\utils.py:579\u001b[39m, in \u001b[36mvalidate_openai_api_key\u001b[39m\u001b[34m(api_key)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[31mValueError\u001b[39m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:304\u001b[39m, in \u001b[36mWorkflow._start.<locals>._task\u001b[39m\u001b[34m(name, queue, step, config)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mCorrectiveRAGWorkflow.query_result\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    195\u001b[39m index = SummaryIndex.from_documents(documents)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m query_engine = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m result = query_engine.query(query_str)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:378\u001b[39m, in \u001b[36mBaseIndex.as_query_engine\u001b[39m\u001b[34m(self, llm, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m retriever = \u001b[38;5;28mself\u001b[39m.as_retriever(**kwargs)\n\u001b[32m    375\u001b[39m llm = (\n\u001b[32m    376\u001b[39m     resolve_llm(llm, callback_manager=\u001b[38;5;28mself\u001b[39m._callback_manager)\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\n\u001b[32m    379\u001b[39m )\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine.from_args(\n\u001b[32m    382\u001b[39m     retriever,\n\u001b[32m    383\u001b[39m     llm=llm,\n\u001b[32m    384\u001b[39m     **kwargs,\n\u001b[32m    385\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\settings.py:36\u001b[39m, in \u001b[36m_Settings.llm\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mself\u001b[39m._llm = \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:49\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not load OpenAI model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[32m      3\u001b[39m tavily_ai_api_key = \u001b[33m\"\u001b[39m\u001b[33mtvly-dev-9L2W3WEagujeArELnYobzM82CedCLSKB\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m workflow.run(\n\u001b[32m      6\u001b[39m     query_str=\u001b[33m\"\u001b[39m\u001b[33mHow was Llama2 pretrained?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     index=index,\n\u001b[32m      8\u001b[39m     tavily_ai_apikey=tavily_ai_api_key,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m display(Markdown(\u001b[38;5;28mstr\u001b[39m(response)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:542\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    539\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    540\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    545\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    546\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\anaconda3\\envs\\llm_env_3.11\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:311\u001b[39m, in \u001b[36mWorkflow._start.<locals>._task\u001b[39m\u001b[34m(name, queue, step, config)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    312\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    315\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    316\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    317\u001b[39m     )\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'query_result': \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "tavily_ai_api_key = \"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\"\n",
    "\n",
    "response = await workflow.run(\n",
    "    query_str=\"How was Llama2 pretrained?\",\n",
    "    index=index,\n",
    "    tavily_ai_apikey=tavily_ai_api_key,\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await workflow.run(\n",
    "    query_str=\"What is the functionality of latest ChatGPT memory.\"\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
